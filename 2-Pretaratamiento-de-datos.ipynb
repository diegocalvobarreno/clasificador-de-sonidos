{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de sonido ambiente \n",
    "---\n",
    "# Preparación de datos\n",
    "\n",
    "## Extraer características\n",
    "Posteriormente extraemos las características propias de cada imagen que nos van a permitir entrenar el modelo.\n",
    "\n",
    "<p>Para ello vamos a crear una representación visual de cada una de las muestras de audio que nos permitirá identificar características para la clasificación, utilizando las mismas técnicas utilizadas para clasificar imágenes. \n",
    "    \n",
    "<p>Los <strong>espectrogramas</strong> son una técnica útil para visualizar el espectro de frecuencias de un sonido y cómo varían durante un período de tiempo muy corto. Utilizaremos una técnica similar conocida como <a href=\"https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\" target=\"_blank\" rel=\"noreferrer noopener\">Mel-Frequency Cepstral Coefficients (MFCC)</a>.</p>\n",
    "\n",
    "<p>La principal diferencia es que un espectrograma usa una <strong>escala de frecuencia lineal espaciada</strong> (por lo que cada intervalo de frecuencia está espaciado un número igual de Hertz), mientras que un MFCC <strong>usa una escala de frecuencia espaciada cuasi-logarítmica</strong>, que es más similar a cómo el sistema auditivo humano procesa sonidos. </p>\n",
    "\n",
    "<p>La imagen a continuación compara tres representaciones visuales diferentes de una onda de sonido, la primera es la representación en el dominio del tiempo, comparando la amplitud en el tiempo. El siguiente es un espectrograma que muestra la energía en diferentes bandas de frecuencia que cambian con el tiempo, y finalmente un MFCC que podemos ver es muy similar a un espectrograma pero con detalles más distinguibles.</p>\n",
    "<img src=\"https://www.diegocalvo.es/wp-content/uploads/2020/06/image-31-1024x557.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada archivo de audio en el conjunto de datos, extraeremos un MFCC (lo que significa que tenemos una representación de imagen para cada muestra de audio) y lo almacenaremos en un Panda Dataframe junto con su etiqueta de clasificación. Para esto, utilizaremos la función mfcc() de Librosa, que genera un MFCC a partir de datos de audio de series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the full UrbanSound dataset \n",
    "fulldatasetpath = 'audio/'\n",
    "\n",
    "# Set the path to metadata.\n",
    "metadata = pd.read_csv('audio/UrbanSound8K.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_features(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return mfccsscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  40  files\n"
     ]
    }
   ],
   "source": [
    "# Load various imports \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    class_label = row[\"classID\"]\n",
    "    data = extract_features(file_name)\n",
    "    \n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-4.59564667e+02,  1.22800354e+02, -4.79247093e+01,  5.32656937e+01,\n",
      "       -1.13986862e+00,  2.47723713e+01,  7.98013592e+00,  9.67425823e+00,\n",
      "       -1.02354348e+00,  1.12044487e+01, -1.18449421e+01,  1.43469524e+01,\n",
      "       -3.51148415e+00,  1.14010563e+01, -3.67504358e-01,  6.43268776e+00,\n",
      "       -6.60325956e+00,  3.28451920e+00, -2.08006835e+00,  1.09049597e+01,\n",
      "        1.38384926e+00,  6.04806185e+00, -8.37760806e-01,  1.80902553e+00,\n",
      "       -2.65425372e+00,  1.58228743e+00, -1.59489763e+00, -6.00140512e-01,\n",
      "       -1.92130005e+00,  1.28997445e+00,  1.24426317e+00, -1.45356560e+00,\n",
      "       -3.94021797e+00, -2.18472099e+00, -1.45620716e+00, -1.69737124e+00,\n",
      "       -1.49213314e+00,  2.22541404e+00,  1.54661775e+00, -8.36315870e-01],\n",
      "      dtype=float32), 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = featuresdf.loc[1]\n",
    "print( list(features) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir los datos y etiquetas\n",
    "Para transformarlos datos categóricos a numéricos usaremos \"LabelEncoder\" y así conseguiremos que el modelo sea capaz de entenderlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.24686768e+02  1.10562271e+02 -5.41482353e+01 ...  6.17408633e-01\n",
      "  -6.84974134e-01  5.71514487e-01]\n",
      " [-4.59564667e+02  1.22800354e+02 -4.79247093e+01 ...  2.22541404e+00\n",
      "   1.54661775e+00 -8.36315870e-01]\n",
      " [-4.14553772e+02  1.02896904e+02 -3.66649513e+01 ... -1.24488878e+00\n",
      "  -2.17944264e+00 -1.07717764e+00]\n",
      " ...\n",
      " [-3.17793388e+01  1.55154846e+02 -4.89359894e+01 ... -6.00718446e-02\n",
      "  -3.32850844e-01  3.64527069e-02]\n",
      " [-6.25979347e+01  1.88543030e+02 -4.80912743e+01 ... -4.71712708e-01\n",
      "  -3.67541552e-01 -6.41354263e-01]\n",
      " [-5.66862221e+01  1.71027161e+02 -5.16182098e+01 ... -2.69095808e-01\n",
      "  -8.75584364e-01 -2.98762083e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6\n",
      "  7  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir los datos en entrenamiento y test\n",
    "Dividimos el conjunto de datos en dos bloques (80% y 20%) y de ellos sacamos valores de X y de Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 40)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 40)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'x_train' (ndarray)\n",
      "Stored 'x_test' (ndarray)\n",
      "Stored 'y_train' (ndarray)\n",
      "Stored 'y_test' (ndarray)\n",
      "Stored 'yy' (ndarray)\n",
      "Stored 'le' (LabelEncoder)\n",
      "Stored 'metadata' (DataFrame)\n",
      "Stored 'fulldatasetpath' (str)\n"
     ]
    }
   ],
   "source": [
    "### store the preprocessed data for use in the next notebook\n",
    "\n",
    "%store x_train \n",
    "%store x_test \n",
    "%store y_train \n",
    "%store y_test \n",
    "%store yy \n",
    "%store le\n",
    "%store metadata\n",
    "%store fulldatasetpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
